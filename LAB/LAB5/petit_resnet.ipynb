{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: thop in c:\\users\\leami\\anaconda3\\lib\\site-packages (0.1.1.post2209072238)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n"
=======
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[notice] A new release of pip is available: 25.0 -> 25.0.1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\leami\\anaconda3\\lib\\site-packages (from thop) (2.5.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Requirement already satisfied: torch in c:\\users\\leami\\anaconda3\\lib\\site-packages (from thop) (2.5.1)"
=======
      "Requirement already satisfied: filelock in c:\\users\\leami\\anaconda3\\lib\\site-packages (from torch->thop) (3.13.1)"
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Requirement already satisfied: filelock in c:\\users\\leami\\anaconda3\\lib\\site-packages (from torch->thop) (3.13.1)\n",
=======
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from torch->thop) (4.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\leami\\anaconda3\\lib\\site-packages (from torch->thop) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from torch->thop) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\leami\\anaconda3\\lib\\site-packages (from torch->thop) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from torch->thop) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\leami\\anaconda3\\lib\\site-packages (from torch->thop) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from jinja2->torch->thop) (2.1.3)\n",
      "Requirement already satisfied: wandb in c:\\users\\leami\\anaconda3\\lib\\site-packages (0.19.6)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "\n",
=======
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[notice] A new release of pip is available: 25.0 -> 25.0.1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from wandb) (8.1.7)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n"
=======
      "\n"
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)"
=======
      "\n"
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[notice] To update, run: python.exe -m pip install --upgrade pip"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "\n"
=======
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)"
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\leami\\appdata\\roaming\\python\\python312\\site-packages (from wandb) (4.1.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\leami\\appdata\\roaming\\python\\python312\\site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\leami\\anaconda3\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from wandb) (2.32.3)\n",
<<<<<<< HEAD
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from wandb) (2.18.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
=======
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from wandb) (2.18.0)\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "Requirement already satisfied: setproctitle in c:\\users\\leami\\anaconda3\\lib\\site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\leami\\anaconda3\\lib\\site-packages (from wandb) (75.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\leami\\appdata\\roaming\\python\\python312\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\leami\\appdata\\roaming\\python\\python312\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\leami\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install thop\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 24,
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from thop import profile\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
=======
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
     "data": {
      "text/plain": [
       "True"
      ]
     },
<<<<<<< HEAD
     "execution_count": 12,
=======
     "execution_count": 25,
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 26,
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, groups=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, \n",
    "                               padding=1, bias=False, groups=groups)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, \n",
    "                               padding=1, bias=False, groups=groups)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, \n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, groups=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, groups=groups)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, groups=groups)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, groups=groups)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, groups=groups)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride, groups):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, groups=groups))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "class mini_ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, groups=1):\n",
    "        super().__init__()  # ✅ Correction ici\n",
    "        self.in_planes = 32  # Commence avec une largeur réduite\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1, groups=groups)\n",
    "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2, groups=groups)\n",
    "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2, groups=groups)\n",
    "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2, groups=groups)\n",
    "        self.linear = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride, groups):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, groups=groups))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet8(groups=8):\n",
    "    return ResNet(BasicBlock, [1, 1, 1, 1], groups=groups)  # 8 blocs seulement\n",
    "\n",
    "def mini_ResNet8(groups=8):\n",
    "    return mini_ResNet(BasicBlock, [1, 1, 1, 1], groups=groups)  # 8 blocs seulement\n",
<<<<<<< HEAD
    "\n",
    "def mini_ResNet18(groups=8):\n",
    "    return mini_ResNet(BasicBlock, [2, 2, 2, 2], groups=groups)  # 16 blocs seulement\n",
    "\n",
    "def ResNet10(groups=8):\n",
    "    return ResNet(BasicBlock, [1, 1, 2, 2], groups=groups)  # 10 blocs seulement\n",
=======
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 27,
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
<<<<<<< HEAD
      "TRAINING: Epoch [1/70], Loss: 1.4012, Accuracy: 48.9940, Learning rate: 0.001\n",
=======
      "TRAINING: Epoch [1/70], Loss: 1.4746, Accuracy: 45.9360, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 1: Train Loss: 1.4012, Train Acc: 48.99%, Test Loss: 1.1045, Test Acc: 60.92% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [2/70], Loss: 1.0461, Accuracy: 62.8500, Learning rate: 0.001\n",
=======
      "Epoch 1: Train Loss: 1.4746, Train Acc: 45.94%, Test Loss: 1.2484, Test Acc: 56.06% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [2/70], Loss: 1.1369, Accuracy: 59.0820, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 2: Train Loss: 1.0461, Train Acc: 62.85%, Test Loss: 0.9413, Test Acc: 67.16% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [3/70], Loss: 0.9060, Accuracy: 68.1560, Learning rate: 0.001\n",
=======
      "Epoch 2: Train Loss: 1.1369, Train Acc: 59.08%, Test Loss: 1.0960, Test Acc: 61.40% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [3/70], Loss: 1.0212, Accuracy: 63.6980, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 3: Train Loss: 0.9060, Train Acc: 68.16%, Test Loss: 0.7840, Test Acc: 72.69% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [4/70], Loss: 0.8159, Accuracy: 71.7000, Learning rate: 0.001\n",
=======
      "Epoch 3: Train Loss: 1.0212, Train Acc: 63.70%, Test Loss: 0.9807, Test Acc: 66.21% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [4/70], Loss: 0.9494, Accuracy: 66.4100, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 4: Train Loss: 0.8159, Train Acc: 71.70%, Test Loss: 0.8339, Test Acc: 70.83% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [5/70], Loss: 0.7668, Accuracy: 73.2160, Learning rate: 0.001\n",
=======
      "Epoch 4: Train Loss: 0.9494, Train Acc: 66.41%, Test Loss: 0.8702, Test Acc: 69.88% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [5/70], Loss: 0.8919, Accuracy: 68.4660, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 5: Train Loss: 0.7668, Train Acc: 73.22%, Test Loss: 0.6990, Test Acc: 76.28% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [6/70], Loss: 0.7144, Accuracy: 75.1960, Learning rate: 0.001\n",
=======
      "Epoch 5: Train Loss: 0.8919, Train Acc: 68.47%, Test Loss: 0.8776, Test Acc: 69.80% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [6/70], Loss: 0.8480, Accuracy: 70.3300, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 6: Train Loss: 0.7144, Train Acc: 75.20%, Test Loss: 0.6539, Test Acc: 77.85% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [7/70], Loss: 0.6798, Accuracy: 76.3460, Learning rate: 0.001\n",
=======
      "Epoch 6: Train Loss: 0.8480, Train Acc: 70.33%, Test Loss: 0.7704, Test Acc: 73.36% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [7/70], Loss: 0.8134, Accuracy: 71.4740, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 7: Train Loss: 0.6798, Train Acc: 76.35%, Test Loss: 0.6183, Test Acc: 78.60% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [8/70], Loss: 0.6545, Accuracy: 77.3940, Learning rate: 0.001\n",
=======
      "Epoch 7: Train Loss: 0.8134, Train Acc: 71.47%, Test Loss: 0.7912, Test Acc: 72.64% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [8/70], Loss: 0.7804, Accuracy: 72.6780, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 8: Train Loss: 0.6545, Train Acc: 77.39%, Test Loss: 0.6092, Test Acc: 78.90% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [9/70], Loss: 0.6224, Accuracy: 78.3920, Learning rate: 0.001\n",
=======
      "Epoch 8: Train Loss: 0.7804, Train Acc: 72.68%, Test Loss: 0.7163, Test Acc: 74.94% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [9/70], Loss: 0.7623, Accuracy: 73.4320, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 9: Train Loss: 0.6224, Train Acc: 78.39%, Test Loss: 0.5861, Test Acc: 79.02% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [10/70], Loss: 0.6044, Accuracy: 79.2280, Learning rate: 0.001\n",
=======
      "Epoch 9: Train Loss: 0.7623, Train Acc: 73.43%, Test Loss: 0.6956, Test Acc: 75.47% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [10/70], Loss: 0.7332, Accuracy: 74.4860, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 10: Train Loss: 0.6044, Train Acc: 79.23%, Test Loss: 0.6181, Test Acc: 78.60% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [11/70], Loss: 0.5874, Accuracy: 79.7220, Learning rate: 0.001\n",
=======
      "Epoch 10: Train Loss: 0.7332, Train Acc: 74.49%, Test Loss: 0.7059, Test Acc: 75.81% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [11/70], Loss: 0.7183, Accuracy: 74.9720, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 11: Train Loss: 0.5874, Train Acc: 79.72%, Test Loss: 0.5739, Test Acc: 80.36% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [12/70], Loss: 0.5740, Accuracy: 80.2900, Learning rate: 0.001\n",
=======
      "Epoch 11: Train Loss: 0.7183, Train Acc: 74.97%, Test Loss: 0.6620, Test Acc: 77.32% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [12/70], Loss: 0.7116, Accuracy: 75.1700, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 12: Train Loss: 0.5740, Train Acc: 80.29%, Test Loss: 0.5292, Test Acc: 81.45% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [13/70], Loss: 0.5562, Accuracy: 81.0180, Learning rate: 0.001\n",
=======
      "Epoch 12: Train Loss: 0.7116, Train Acc: 75.17%, Test Loss: 0.6708, Test Acc: 76.93% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [13/70], Loss: 0.6908, Accuracy: 75.7460, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 13: Train Loss: 0.5562, Train Acc: 81.02%, Test Loss: 0.5099, Test Acc: 82.54% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [14/70], Loss: 0.5520, Accuracy: 80.9360, Learning rate: 0.001\n",
=======
      "Epoch 13: Train Loss: 0.6908, Train Acc: 75.75%, Test Loss: 0.6897, Test Acc: 76.41% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [14/70], Loss: 0.6817, Accuracy: 76.1300, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 14: Train Loss: 0.5520, Train Acc: 80.94%, Test Loss: 0.5104, Test Acc: 82.30% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [15/70], Loss: 0.5399, Accuracy: 81.3480, Learning rate: 0.001\n",
=======
      "Epoch 14: Train Loss: 0.6817, Train Acc: 76.13%, Test Loss: 0.6296, Test Acc: 78.30% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [15/70], Loss: 0.6636, Accuracy: 76.8280, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 15: Train Loss: 0.5399, Train Acc: 81.35%, Test Loss: 0.5517, Test Acc: 81.10% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [16/70], Loss: 0.5269, Accuracy: 81.7560, Learning rate: 0.001\n",
=======
      "Epoch 15: Train Loss: 0.6636, Train Acc: 76.83%, Test Loss: 0.6435, Test Acc: 78.12% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [16/70], Loss: 0.6592, Accuracy: 76.9400, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 16: Train Loss: 0.5269, Train Acc: 81.76%, Test Loss: 0.4869, Test Acc: 83.26% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [17/70], Loss: 0.5160, Accuracy: 82.2520, Learning rate: 0.001\n",
=======
      "Epoch 16: Train Loss: 0.6592, Train Acc: 76.94%, Test Loss: 0.6000, Test Acc: 79.38% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [17/70], Loss: 0.6504, Accuracy: 77.4580, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 17: Train Loss: 0.5160, Train Acc: 82.25%, Test Loss: 0.4838, Test Acc: 83.39% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [18/70], Loss: 0.5177, Accuracy: 82.0960, Learning rate: 0.001\n",
=======
      "Epoch 17: Train Loss: 0.6504, Train Acc: 77.46%, Test Loss: 0.6289, Test Acc: 78.56% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [18/70], Loss: 0.6427, Accuracy: 77.7620, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 18: Train Loss: 0.5177, Train Acc: 82.10%, Test Loss: 0.4740, Test Acc: 83.64% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [19/70], Loss: 0.5105, Accuracy: 82.3180, Learning rate: 0.001\n",
=======
      "Epoch 18: Train Loss: 0.6427, Train Acc: 77.76%, Test Loss: 0.6057, Test Acc: 79.27% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [19/70], Loss: 0.6390, Accuracy: 77.7880, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 19: Train Loss: 0.5105, Train Acc: 82.32%, Test Loss: 0.4725, Test Acc: 84.22% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [20/70], Loss: 0.5055, Accuracy: 82.4560, Learning rate: 0.001\n",
=======
      "Epoch 19: Train Loss: 0.6390, Train Acc: 77.79%, Test Loss: 0.6114, Test Acc: 78.86% Learning Rate: 0.001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [20/70], Loss: 0.6316, Accuracy: 78.0100, Learning rate: 0.001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 20: Train Loss: 0.5055, Train Acc: 82.46%, Test Loss: 0.5296, Test Acc: 82.05% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [21/70], Loss: 0.4996, Accuracy: 82.6440, Learning rate: 0.001\n",
=======
      "Epoch 20: Train Loss: 0.6316, Train Acc: 78.01%, Test Loss: 0.6054, Test Acc: 79.53% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [21/70], Loss: 0.5512, Accuracy: 80.8880, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 21: Train Loss: 0.4996, Train Acc: 82.64%, Test Loss: 0.4787, Test Acc: 84.14% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [22/70], Loss: 0.4910, Accuracy: 82.9620, Learning rate: 0.001\n",
=======
      "Epoch 21: Train Loss: 0.5512, Train Acc: 80.89%, Test Loss: 0.5079, Test Acc: 82.89% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [22/70], Loss: 0.5267, Accuracy: 81.9300, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 22: Train Loss: 0.4910, Train Acc: 82.96%, Test Loss: 0.5469, Test Acc: 81.66% Learning Rate: 0.001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [23/70], Loss: 0.4936, Accuracy: 82.9780, Learning rate: 0.001\n",
=======
      "Epoch 22: Train Loss: 0.5267, Train Acc: 81.93%, Test Loss: 0.5001, Test Acc: 83.12% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [23/70], Loss: 0.5150, Accuracy: 82.2060, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 23: Train Loss: 0.4936, Train Acc: 82.98%, Test Loss: 0.4765, Test Acc: 84.08% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [24/70], Loss: 0.3903, Accuracy: 86.7580, Learning rate: 0.0001\n",
=======
      "Epoch 23: Train Loss: 0.5150, Train Acc: 82.21%, Test Loss: 0.4949, Test Acc: 83.17% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [24/70], Loss: 0.5134, Accuracy: 82.2580, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 24: Train Loss: 0.3903, Train Acc: 86.76%, Test Loss: 0.3577, Test Acc: 87.77% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [25/70], Loss: 0.3564, Accuracy: 87.6860, Learning rate: 0.0001\n",
=======
      "Epoch 24: Train Loss: 0.5134, Train Acc: 82.26%, Test Loss: 0.4956, Test Acc: 83.39% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [25/70], Loss: 0.5056, Accuracy: 82.6280, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 25: Train Loss: 0.3564, Train Acc: 87.69%, Test Loss: 0.3546, Test Acc: 87.75% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [26/70], Loss: 0.3402, Accuracy: 88.4680, Learning rate: 0.0001\n",
=======
      "Epoch 25: Train Loss: 0.5056, Train Acc: 82.63%, Test Loss: 0.4958, Test Acc: 83.46% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [26/70], Loss: 0.5007, Accuracy: 82.6400, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 26: Train Loss: 0.3402, Train Acc: 88.47%, Test Loss: 0.3498, Test Acc: 88.12% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [27/70], Loss: 0.3346, Accuracy: 88.4120, Learning rate: 0.0001\n",
=======
      "Epoch 26: Train Loss: 0.5007, Train Acc: 82.64%, Test Loss: 0.4866, Test Acc: 83.54% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [27/70], Loss: 0.4952, Accuracy: 83.0180, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 27: Train Loss: 0.3346, Train Acc: 88.41%, Test Loss: 0.3437, Test Acc: 88.13% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [28/70], Loss: 0.3232, Accuracy: 88.9880, Learning rate: 0.0001\n",
=======
      "Epoch 27: Train Loss: 0.4952, Train Acc: 83.02%, Test Loss: 0.4826, Test Acc: 83.73% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [28/70], Loss: 0.4954, Accuracy: 82.7740, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 28: Train Loss: 0.3232, Train Acc: 88.99%, Test Loss: 0.3434, Test Acc: 88.24% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [29/70], Loss: 0.3152, Accuracy: 89.0960, Learning rate: 0.0001\n",
=======
      "Epoch 28: Train Loss: 0.4954, Train Acc: 82.77%, Test Loss: 0.4842, Test Acc: 83.57% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [29/70], Loss: 0.4903, Accuracy: 82.9840, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 29: Train Loss: 0.3152, Train Acc: 89.10%, Test Loss: 0.3371, Test Acc: 88.34% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [30/70], Loss: 0.3114, Accuracy: 89.3620, Learning rate: 0.0001\n",
=======
      "Epoch 29: Train Loss: 0.4903, Train Acc: 82.98%, Test Loss: 0.4773, Test Acc: 83.80% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [30/70], Loss: 0.4850, Accuracy: 83.3020, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 30: Train Loss: 0.3114, Train Acc: 89.36%, Test Loss: 0.3284, Test Acc: 88.90% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [31/70], Loss: 0.3061, Accuracy: 89.2440, Learning rate: 0.0001\n",
=======
      "Epoch 30: Train Loss: 0.4850, Train Acc: 83.30%, Test Loss: 0.4743, Test Acc: 83.96% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [31/70], Loss: 0.4830, Accuracy: 83.1720, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 31: Train Loss: 0.3061, Train Acc: 89.24%, Test Loss: 0.3276, Test Acc: 89.22% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [32/70], Loss: 0.2974, Accuracy: 89.6660, Learning rate: 0.0001\n",
=======
      "Epoch 31: Train Loss: 0.4830, Train Acc: 83.17%, Test Loss: 0.4727, Test Acc: 83.86% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [32/70], Loss: 0.4789, Accuracy: 83.5280, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 32: Train Loss: 0.2974, Train Acc: 89.67%, Test Loss: 0.3266, Test Acc: 88.91% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [33/70], Loss: 0.2940, Accuracy: 89.8880, Learning rate: 0.0001\n",
=======
      "Epoch 32: Train Loss: 0.4789, Train Acc: 83.53%, Test Loss: 0.4745, Test Acc: 84.29% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [33/70], Loss: 0.4741, Accuracy: 83.4760, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 33: Train Loss: 0.2940, Train Acc: 89.89%, Test Loss: 0.3239, Test Acc: 88.89% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [34/70], Loss: 0.2877, Accuracy: 90.0000, Learning rate: 0.0001\n",
=======
      "Epoch 33: Train Loss: 0.4741, Train Acc: 83.48%, Test Loss: 0.4683, Test Acc: 84.00% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [34/70], Loss: 0.4722, Accuracy: 83.6500, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 34: Train Loss: 0.2877, Train Acc: 90.00%, Test Loss: 0.3259, Test Acc: 88.93% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [35/70], Loss: 0.2848, Accuracy: 90.0620, Learning rate: 0.0001\n",
=======
      "Epoch 34: Train Loss: 0.4722, Train Acc: 83.65%, Test Loss: 0.4687, Test Acc: 84.16% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [35/70], Loss: 0.4734, Accuracy: 83.7020, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 35: Train Loss: 0.2848, Train Acc: 90.06%, Test Loss: 0.3295, Test Acc: 89.02% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [36/70], Loss: 0.2784, Accuracy: 90.3440, Learning rate: 0.0001\n",
=======
      "Epoch 35: Train Loss: 0.4734, Train Acc: 83.70%, Test Loss: 0.4726, Test Acc: 84.01% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [36/70], Loss: 0.4700, Accuracy: 83.7020, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 36: Train Loss: 0.2784, Train Acc: 90.34%, Test Loss: 0.3237, Test Acc: 89.22% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [37/70], Loss: 0.2749, Accuracy: 90.5380, Learning rate: 0.0001\n",
=======
      "Epoch 36: Train Loss: 0.4700, Train Acc: 83.70%, Test Loss: 0.4654, Test Acc: 83.96% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [37/70], Loss: 0.4645, Accuracy: 83.9080, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 37: Train Loss: 0.2749, Train Acc: 90.54%, Test Loss: 0.3230, Test Acc: 89.57% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [38/70], Loss: 0.2690, Accuracy: 90.7160, Learning rate: 0.0001\n",
=======
      "Epoch 37: Train Loss: 0.4645, Train Acc: 83.91%, Test Loss: 0.4618, Test Acc: 84.35% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [38/70], Loss: 0.4626, Accuracy: 83.9520, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 38: Train Loss: 0.2690, Train Acc: 90.72%, Test Loss: 0.3182, Test Acc: 89.44% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [39/70], Loss: 0.2686, Accuracy: 90.6080, Learning rate: 0.0001\n",
=======
      "Epoch 38: Train Loss: 0.4626, Train Acc: 83.95%, Test Loss: 0.4637, Test Acc: 84.44% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [39/70], Loss: 0.4594, Accuracy: 84.0100, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 39: Train Loss: 0.2686, Train Acc: 90.61%, Test Loss: 0.3137, Test Acc: 89.40% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [40/70], Loss: 0.2650, Accuracy: 90.7440, Learning rate: 0.0001\n",
=======
      "Epoch 39: Train Loss: 0.4594, Train Acc: 84.01%, Test Loss: 0.4613, Test Acc: 84.53% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [40/70], Loss: 0.4571, Accuracy: 84.2640, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 40: Train Loss: 0.2650, Train Acc: 90.74%, Test Loss: 0.3232, Test Acc: 89.16% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [41/70], Loss: 0.2576, Accuracy: 91.0700, Learning rate: 0.0001\n",
=======
      "Epoch 40: Train Loss: 0.4571, Train Acc: 84.26%, Test Loss: 0.4602, Test Acc: 84.57% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [41/70], Loss: 0.4558, Accuracy: 84.3240, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 41: Train Loss: 0.2576, Train Acc: 91.07%, Test Loss: 0.3207, Test Acc: 89.34% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [42/70], Loss: 0.2597, Accuracy: 90.9840, Learning rate: 0.0001\n",
=======
      "Epoch 41: Train Loss: 0.4558, Train Acc: 84.32%, Test Loss: 0.4558, Test Acc: 84.50% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [42/70], Loss: 0.4581, Accuracy: 84.1300, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 42: Train Loss: 0.2597, Train Acc: 90.98%, Test Loss: 0.3229, Test Acc: 89.22% Learning Rate: 0.0001Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [43/70], Loss: 0.2588, Accuracy: 90.9600, Learning rate: 0.0001\n",
=======
      "Epoch 42: Train Loss: 0.4581, Train Acc: 84.13%, Test Loss: 0.4629, Test Acc: 84.25% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [43/70], Loss: 0.4576, Accuracy: 84.2640, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 43: Train Loss: 0.2588, Train Acc: 90.96%, Test Loss: 0.3149, Test Acc: 89.50% Learning Rate: 1e-05Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [44/70], Loss: 0.2410, Accuracy: 91.7560, Learning rate: 1e-05\n",
=======
      "Epoch 43: Train Loss: 0.4576, Train Acc: 84.26%, Test Loss: 0.4532, Test Acc: 84.66% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [44/70], Loss: 0.4554, Accuracy: 84.1500, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 44: Train Loss: 0.2410, Train Acc: 91.76%, Test Loss: 0.3058, Test Acc: 89.74% Learning Rate: 1e-05Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [45/70], Loss: 0.2341, Accuracy: 91.8440, Learning rate: 1e-05\n",
=======
      "Epoch 44: Train Loss: 0.4554, Train Acc: 84.15%, Test Loss: 0.4606, Test Acc: 84.55% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [45/70], Loss: 0.4504, Accuracy: 84.2440, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 45: Train Loss: 0.2341, Train Acc: 91.84%, Test Loss: 0.3033, Test Acc: 89.87% Learning Rate: 1e-05Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [46/70], Loss: 0.2335, Accuracy: 91.9820, Learning rate: 1e-05\n",
=======
      "Epoch 45: Train Loss: 0.4504, Train Acc: 84.24%, Test Loss: 0.4538, Test Acc: 84.56% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [46/70], Loss: 0.4504, Accuracy: 84.4460, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 46: Train Loss: 0.2335, Train Acc: 91.98%, Test Loss: 0.3008, Test Acc: 89.94% Learning Rate: 1e-05Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [47/70], Loss: 0.2289, Accuracy: 92.1320, Learning rate: 1e-05\n",
=======
      "Epoch 46: Train Loss: 0.4504, Train Acc: 84.45%, Test Loss: 0.4548, Test Acc: 84.58% Learning Rate: 0.0001Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [47/70], Loss: 0.4451, Accuracy: 84.6180, Learning rate: 0.0001\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 47: Train Loss: 0.2289, Train Acc: 92.13%, Test Loss: 0.2988, Test Acc: 90.09% Learning Rate: 1e-05Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [48/70], Loss: 0.2291, Accuracy: 92.1600, Learning rate: 1e-05\n",
=======
      "Epoch 47: Train Loss: 0.4451, Train Acc: 84.62%, Test Loss: 0.4583, Test Acc: 84.55% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [48/70], Loss: 0.4395, Accuracy: 84.8020, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 48: Train Loss: 0.2291, Train Acc: 92.16%, Test Loss: 0.3039, Test Acc: 89.88% Learning Rate: 1e-05Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [49/70], Loss: 0.2346, Accuracy: 91.8560, Learning rate: 1e-05\n",
=======
      "Epoch 48: Train Loss: 0.4395, Train Acc: 84.80%, Test Loss: 0.4468, Test Acc: 84.75% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [49/70], Loss: 0.4352, Accuracy: 84.9140, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 49: Train Loss: 0.2346, Train Acc: 91.86%, Test Loss: 0.3007, Test Acc: 90.05% Learning Rate: 1e-05Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [50/70], Loss: 0.2248, Accuracy: 92.2220, Learning rate: 1e-05\n",
=======
      "Epoch 49: Train Loss: 0.4352, Train Acc: 84.91%, Test Loss: 0.4507, Test Acc: 84.69% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [50/70], Loss: 0.4344, Accuracy: 84.9020, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 50: Train Loss: 0.2248, Train Acc: 92.22%, Test Loss: 0.3061, Test Acc: 89.81% Learning Rate: 1e-05Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [51/70], Loss: 0.2272, Accuracy: 92.1680, Learning rate: 1e-05\n",
=======
      "Epoch 50: Train Loss: 0.4344, Train Acc: 84.90%, Test Loss: 0.4457, Test Acc: 84.88% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [51/70], Loss: 0.4318, Accuracy: 85.1440, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 51: Train Loss: 0.2272, Train Acc: 92.17%, Test Loss: 0.3021, Test Acc: 89.95% Learning Rate: 1.0000000000000002e-06Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [52/70], Loss: 0.2226, Accuracy: 92.2800, Learning rate: 1.0000000000000002e-06\n",
=======
      "Epoch 51: Train Loss: 0.4318, Train Acc: 85.14%, Test Loss: 0.4446, Test Acc: 84.74% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [52/70], Loss: 0.4265, Accuracy: 85.2540, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 52: Train Loss: 0.2226, Train Acc: 92.28%, Test Loss: 0.3024, Test Acc: 90.04% Learning Rate: 1.0000000000000002e-06Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [53/70], Loss: 0.2250, Accuracy: 92.1740, Learning rate: 1.0000000000000002e-06\n",
=======
      "Epoch 52: Train Loss: 0.4265, Train Acc: 85.25%, Test Loss: 0.4453, Test Acc: 85.09% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [53/70], Loss: 0.4286, Accuracy: 85.2660, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 53: Train Loss: 0.2250, Train Acc: 92.17%, Test Loss: 0.3031, Test Acc: 89.89% Learning Rate: 1.0000000000000002e-06Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [54/70], Loss: 0.2246, Accuracy: 92.1320, Learning rate: 1.0000000000000002e-06\n",
=======
      "Epoch 53: Train Loss: 0.4286, Train Acc: 85.27%, Test Loss: 0.4507, Test Acc: 84.80% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [54/70], Loss: 0.4303, Accuracy: 85.1500, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 54: Train Loss: 0.2246, Train Acc: 92.13%, Test Loss: 0.3016, Test Acc: 89.98% Learning Rate: 1.0000000000000002e-06Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [55/70], Loss: 0.2253, Accuracy: 92.2480, Learning rate: 1.0000000000000002e-06\n",
=======
      "Epoch 54: Train Loss: 0.4303, Train Acc: 85.15%, Test Loss: 0.4445, Test Acc: 84.90% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [55/70], Loss: 0.4311, Accuracy: 85.0520, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 55: Train Loss: 0.2253, Train Acc: 92.25%, Test Loss: 0.3067, Test Acc: 89.64% Learning Rate: 1.0000000000000002e-07Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [56/70], Loss: 0.2243, Accuracy: 92.3260, Learning rate: 1.0000000000000002e-07\n",
=======
      "Epoch 55: Train Loss: 0.4311, Train Acc: 85.05%, Test Loss: 0.4422, Test Acc: 85.08% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [56/70], Loss: 0.4300, Accuracy: 85.0840, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 56: Train Loss: 0.2243, Train Acc: 92.33%, Test Loss: 0.3038, Test Acc: 89.79% Learning Rate: 1.0000000000000002e-07Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [57/70], Loss: 0.2262, Accuracy: 92.2880, Learning rate: 1.0000000000000002e-07\n",
=======
      "Epoch 56: Train Loss: 0.4300, Train Acc: 85.08%, Test Loss: 0.4420, Test Acc: 85.04% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [57/70], Loss: 0.4328, Accuracy: 85.0260, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 57: Train Loss: 0.2262, Train Acc: 92.29%, Test Loss: 0.3017, Test Acc: 89.98% Learning Rate: 1.0000000000000002e-07Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [58/70], Loss: 0.2239, Accuracy: 92.0200, Learning rate: 1.0000000000000002e-07\n",
=======
      "Epoch 57: Train Loss: 0.4328, Train Acc: 85.03%, Test Loss: 0.4435, Test Acc: 84.91% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [58/70], Loss: 0.4278, Accuracy: 85.1440, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 58: Train Loss: 0.2239, Train Acc: 92.02%, Test Loss: 0.3005, Test Acc: 90.00% Learning Rate: 1.0000000000000002e-07Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [59/70], Loss: 0.2213, Accuracy: 92.3800, Learning rate: 1.0000000000000002e-07\n",
=======
      "Epoch 58: Train Loss: 0.4278, Train Acc: 85.14%, Test Loss: 0.4431, Test Acc: 84.89% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [59/70], Loss: 0.4284, Accuracy: 85.2640, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 59: Train Loss: 0.2213, Train Acc: 92.38%, Test Loss: 0.3087, Test Acc: 89.73% Learning Rate: 1.0000000000000004e-08Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [60/70], Loss: 0.2197, Accuracy: 92.4480, Learning rate: 1.0000000000000004e-08\n",
=======
      "Epoch 59: Train Loss: 0.4284, Train Acc: 85.26%, Test Loss: 0.4435, Test Acc: 84.79% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [60/70], Loss: 0.4220, Accuracy: 85.5820, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 60: Train Loss: 0.2197, Train Acc: 92.45%, Test Loss: 0.3035, Test Acc: 89.85% Learning Rate: 1.0000000000000004e-08Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [61/70], Loss: 0.2239, Accuracy: 92.2600, Learning rate: 1.0000000000000004e-08\n",
=======
      "Epoch 60: Train Loss: 0.4220, Train Acc: 85.58%, Test Loss: 0.4407, Test Acc: 85.08% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [61/70], Loss: 0.4254, Accuracy: 85.3980, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 61: Train Loss: 0.2239, Train Acc: 92.26%, Test Loss: 0.3048, Test Acc: 89.96% Learning Rate: 1.0000000000000004e-08Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [62/70], Loss: 0.2232, Accuracy: 92.2060, Learning rate: 1.0000000000000004e-08\n",
=======
      "Epoch 61: Train Loss: 0.4254, Train Acc: 85.40%, Test Loss: 0.4429, Test Acc: 85.00% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [62/70], Loss: 0.4253, Accuracy: 85.4080, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 62: Train Loss: 0.2232, Train Acc: 92.21%, Test Loss: 0.3055, Test Acc: 89.76% Learning Rate: 1.0000000000000004e-08Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [63/70], Loss: 0.2268, Accuracy: 92.1500, Learning rate: 1.0000000000000004e-08\n",
=======
      "Epoch 62: Train Loss: 0.4253, Train Acc: 85.41%, Test Loss: 0.4419, Test Acc: 84.89% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [63/70], Loss: 0.4284, Accuracy: 85.2060, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 63: Train Loss: 0.2268, Train Acc: 92.15%, Test Loss: 0.3025, Test Acc: 89.95% Learning Rate: 1.0000000000000004e-08Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [64/70], Loss: 0.2228, Accuracy: 92.2560, Learning rate: 1.0000000000000004e-08\n",
=======
      "Epoch 63: Train Loss: 0.4284, Train Acc: 85.21%, Test Loss: 0.4420, Test Acc: 84.99% Learning Rate: 1e-05Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [64/70], Loss: 0.4258, Accuracy: 85.0940, Learning rate: 1e-05\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 64: Train Loss: 0.2228, Train Acc: 92.26%, Test Loss: 0.3022, Test Acc: 89.97% Learning Rate: 1.0000000000000004e-08Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [65/70], Loss: 0.2283, Accuracy: 91.8860, Learning rate: 1.0000000000000004e-08\n",
=======
      "Epoch 64: Train Loss: 0.4258, Train Acc: 85.09%, Test Loss: 0.4445, Test Acc: 84.93% Learning Rate: 1.0000000000000002e-06Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [65/70], Loss: 0.4246, Accuracy: 85.2880, Learning rate: 1.0000000000000002e-06\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 65: Train Loss: 0.2283, Train Acc: 91.89%, Test Loss: 0.3044, Test Acc: 89.81% Learning Rate: 1.0000000000000004e-08Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [66/70], Loss: 0.2274, Accuracy: 92.2480, Learning rate: 1.0000000000000004e-08\n",
=======
      "Epoch 65: Train Loss: 0.4246, Train Acc: 85.29%, Test Loss: 0.4420, Test Acc: 84.92% Learning Rate: 1.0000000000000002e-06Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [66/70], Loss: 0.4231, Accuracy: 85.3280, Learning rate: 1.0000000000000002e-06\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 66: Train Loss: 0.2274, Train Acc: 92.25%, Test Loss: 0.2988, Test Acc: 90.00% Learning Rate: 1.0000000000000004e-08Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [67/70], Loss: 0.2261, Accuracy: 92.2000, Learning rate: 1.0000000000000004e-08\n",
=======
      "Epoch 66: Train Loss: 0.4231, Train Acc: 85.33%, Test Loss: 0.4427, Test Acc: 84.97% Learning Rate: 1.0000000000000002e-06Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [67/70], Loss: 0.4227, Accuracy: 85.2620, Learning rate: 1.0000000000000002e-06\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 67: Train Loss: 0.2261, Train Acc: 92.20%, Test Loss: 0.3037, Test Acc: 89.76% Learning Rate: 1.0000000000000004e-08Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [68/70], Loss: 0.2252, Accuracy: 92.3180, Learning rate: 1.0000000000000004e-08\n",
=======
      "Epoch 67: Train Loss: 0.4227, Train Acc: 85.26%, Test Loss: 0.4425, Test Acc: 84.90% Learning Rate: 1.0000000000000002e-06Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [68/70], Loss: 0.4269, Accuracy: 85.1400, Learning rate: 1.0000000000000002e-06\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 68: Train Loss: 0.2252, Train Acc: 92.32%, Test Loss: 0.3039, Test Acc: 89.85% Learning Rate: 1.0000000000000004e-08Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [69/70], Loss: 0.2203, Accuracy: 92.4180, Learning rate: 1.0000000000000004e-08\n",
=======
      "Epoch 68: Train Loss: 0.4269, Train Acc: 85.14%, Test Loss: 0.4432, Test Acc: 84.94% Learning Rate: 1.0000000000000002e-07Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [69/70], Loss: 0.4213, Accuracy: 85.3420, Learning rate: 1.0000000000000002e-07\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 69: Train Loss: 0.2203, Train Acc: 92.42%, Test Loss: 0.3017, Test Acc: 90.09% Learning Rate: 1.0000000000000004e-08Num params: 2841930Num operations: 108827648.0\n",
      "TRAINING: Epoch [70/70], Loss: 0.2264, Accuracy: 92.2140, Learning rate: 1.0000000000000004e-08\n",
=======
      "Epoch 69: Train Loss: 0.4213, Train Acc: 85.34%, Test Loss: 0.4445, Test Acc: 84.81% Learning Rate: 1.0000000000000002e-07Num params: 196778Num operations: 10865152.0\n",
      "TRAINING: Epoch [70/70], Loss: 0.4217, Accuracy: 85.3480, Learning rate: 1.0000000000000002e-07\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
<<<<<<< HEAD
      "Epoch 70: Train Loss: 0.2264, Train Acc: 92.21%, Test Loss: 0.3008, Test Acc: 90.06% Learning Rate: 1.0000000000000004e-08Num params: 2841930Num operations: 108827648.0\n",
=======
      "Epoch 70: Train Loss: 0.4217, Train Acc: 85.35%, Test Loss: 0.4433, Test Acc: 84.94% Learning Rate: 1.0000000000000002e-07Num params: 196778Num operations: 10865152.0\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      "Training complete my boss\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
<<<<<<< HEAD
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>███████████████▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Num operations</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Num params</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Test Accuracy</td><td>▁▂▃▃▅▅▆▆▆▆▆▆▆▇▇█████████████████████████</td></tr><tr><td>Test Loss</td><td>█▇▆█▇▄▃▃▃▃▃▃▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▃▄▁▃▄▅▅▆▆▆▆▆▆▆▆▇▇███████████████████████</td></tr><tr><td>Train Loss</td><td>█▇▆█▇▆▅▅▄▄▄▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>0.0</td></tr><tr><td>Num operations</td><td>108827648.0</td></tr><tr><td>Num params</td><td>2841930</td></tr><tr><td>Test Accuracy</td><td>90.06</td></tr><tr><td>Test Loss</td><td>0.3008</td></tr><tr><td>Train Accuracy</td><td>92.214</td></tr><tr><td>Train Loss</td><td>0.22645</td></tr></table><br/></div></div>"
=======
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>█████████▂▂▂▂▂▂▂▁▁▁▁▁▁▁██▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Num operations</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Num params</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Test Accuracy</td><td>▃▃▄▅▆▆▆▇████████████▁▂▃▅▆▆▇▇████████████</td></tr><tr><td>Test Loss</td><td>▇▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▅▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▅▅▆▆▆▇▇▇▇█████████████▁▃▅▆▆▇▇██████████</td></tr><tr><td>Train Loss</td><td>▆▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁█▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>0.0</td></tr><tr><td>Num operations</td><td>10865152.0</td></tr><tr><td>Num params</td><td>196778</td></tr><tr><td>Test Accuracy</td><td>84.94</td></tr><tr><td>Test Loss</td><td>0.44329</td></tr><tr><td>Train Accuracy</td><td>85.348</td></tr><tr><td>Train Loss</td><td>0.42171</td></tr></table><br/></div></div>"
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
<<<<<<< HEAD
       " View run <strong style=\"color:#cdcd00\">resnet10_8</strong> at: <a href='https://wandb.ai/xueyun02-imt-atlantique/deep-learning-lab3/runs/bl1jlin9' target=\"_blank\">https://wandb.ai/xueyun02-imt-atlantique/deep-learning-lab3/runs/bl1jlin9</a><br> View project at: <a href='https://wandb.ai/xueyun02-imt-atlantique/deep-learning-lab3' target=\"_blank\">https://wandb.ai/xueyun02-imt-atlantique/deep-learning-lab3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
=======
       " View run <strong style=\"color:#cdcd00\">miniresnet_8</strong> at: <a href='https://wandb.ai/xueyun02-imt-atlantique/deep-learning-lab3/runs/p58fi1vm' target=\"_blank\">https://wandb.ai/xueyun02-imt-atlantique/deep-learning-lab3/runs/p58fi1vm</a><br> View project at: <a href='https://wandb.ai/xueyun02-imt-atlantique/deep-learning-lab3' target=\"_blank\">https://wandb.ai/xueyun02-imt-atlantique/deep-learning-lab3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
<<<<<<< HEAD
       "Find logs at: <code>.\\wandb\\run-20250324_190052-bl1jlin9\\logs</code>"
=======
       "Find logs at: <code>.\\wandb\\run-20250316_202302-p58fi1vm\\logs</code>"
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Model and training details saved to ./resnet10_8.pth\n"
=======
      "Model and training details saved to ./miniresnet8.pth\n"
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
     ]
    }
   ],
   "source": [
    "def train(data_path , batch_size, learning_rate, \n",
    "          weight_decay, epochs, save_path, \n",
    "          load_path, prate):\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Data preprocessing\n",
    "    normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        normalize_scratch,\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize_scratch,\n",
    "    ])\n",
    "\n",
    "    c10train = CIFAR10(data_path,train=True,download=True,transform=transform_train)\n",
    "    c10test = CIFAR10(data_path,train=False,download=True,transform=transform_test)\n",
    "\n",
    "    trainloader = DataLoader(c10train,batch_size=batch_size,shuffle=True)\n",
    "    testloader = DataLoader(c10test,batch_size=batch_size)\n",
    "\n",
    "    # Charger le modèle groupe_8.pth\n",
<<<<<<< HEAD
    "    model = ResNet10(groups=4)\n",
=======
    "    model = mini_ResNet8(groups=8)\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "    # Initialize Weights & Biases\n",
    "    dict = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"epochs\": epochs,\n",
    "        \"prate\": prate,\n",
    "    }\n",
<<<<<<< HEAD
    "    wandb.init(project=\"deep-learning-lab3\", config=dict , name=\"resnet10_8\", job_type=\"training_test\")\n",
=======
    "    wandb.init(project=\"deep-learning-lab3\", config=dict , name=\"miniresnet_8\", job_type=\"training_test\")\n",
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Training accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_train += predicted.eq(labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        print(f\"TRAINING: Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct_test += predicted.eq(labels).sum().item()\n",
    "                total_test += labels.size(0)\n",
    "\n",
    "        test_accuracy = 100 * (correct_test / total_test)\n",
    "        test_loss= test_loss / len(testloader)\n",
    "\n",
    "        scheduler.step(test_loss / len(testloader))\n",
    "\n",
    "        # number of parameters\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        # number operations\n",
    "        input_tensor = torch.randn(1, 3, 32, 32).to(device)\n",
    "        flops, _ = profile(model, inputs=(input_tensor,))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_accuracy:.2f}%, \"\n",
    "              f\"Test Loss: {test_loss:.4f}, \"\n",
    "              f\"Test Acc: {test_accuracy:.2f}% \"\n",
    "              f\"Learning Rate: {optimizer.param_groups[0]['lr']}\"\n",
    "              f\"Num params: {num_params}\"\n",
    "              f\"Num operations: {flops}\"\n",
    "              \n",
    "              )\n",
    "        wandb.log({\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Train Accuracy\": train_accuracy,\n",
    "            \"Test Loss\": test_loss,\n",
    "            \"Test Accuracy\": test_accuracy,\n",
    "            \"Learning Rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"Num params\": num_params,\n",
    "            \"Num operations\": flops\n",
    "        })\n",
    "        \n",
    "        \n",
    "        \n",
    "    print(\"Training complete my boss\")\n",
    "    \n",
    "    # Save model along with training hyperparameters\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epochs\": epochs,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"learning_rate\": learning_rate,\n",
    "    }\n",
    "    wandb.finish()\n",
    "    model.half()\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Model and training details saved to {save_path}\")\n",
    "\n",
<<<<<<< HEAD
    "train('/opt/img/effdl-cifar10/', 64, 0.001, 0.0005, 70, './resnet10_4.pth', './resnet10_4.pth', 0.2)"
=======
    "train('/opt/img/effdl-cifar10/', 64, 0.001, 0.0005, 70, './miniresnet8.pth', './miniresnet8.pth', 0.2)"
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leami\\AppData\\Local\\Temp\\ipykernel_15684\\3556514596.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_checkpoint = torch.load(load_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING: Epoch [1/10], Loss: 0.2946, Accuracy: 89.8360, Learning rate: 1e-05\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Epoch 1: Train Loss: 0.2946, Train Acc: 89.84%, Test Loss: 0.3381, Test Acc: 88.65% Learning Rate: 1e-05Num params: 394538Num operations: 20793856.0\n",
      "TRAINING: Epoch [2/10], Loss: 0.2869, Accuracy: 90.0880, Learning rate: 1e-05\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Epoch 2: Train Loss: 0.2869, Train Acc: 90.09%, Test Loss: 0.3344, Test Acc: 88.76% Learning Rate: 1e-05Num params: 394538Num operations: 20793856.0\n",
      "TRAINING: Epoch [3/10], Loss: 0.2874, Accuracy: 90.0480, Learning rate: 1e-05\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Epoch 3: Train Loss: 0.2874, Train Acc: 90.05%, Test Loss: 0.3368, Test Acc: 88.72% Learning Rate: 1e-05Num params: 394538Num operations: 20793856.0\n",
      "TRAINING: Epoch [4/10], Loss: 0.2858, Accuracy: 90.0780, Learning rate: 1e-05\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Epoch 4: Train Loss: 0.2858, Train Acc: 90.08%, Test Loss: 0.3332, Test Acc: 88.78% Learning Rate: 1e-05Num params: 394538Num operations: 20793856.0\n",
      "TRAINING: Epoch [5/10], Loss: 0.2791, Accuracy: 90.3360, Learning rate: 1e-05\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Epoch 5: Train Loss: 0.2791, Train Acc: 90.34%, Test Loss: 0.3376, Test Acc: 88.72% Learning Rate: 1e-05Num params: 394538Num operations: 20793856.0\n",
      "TRAINING: Epoch [6/10], Loss: 0.2810, Accuracy: 90.1180, Learning rate: 1e-05\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Epoch 6: Train Loss: 0.2810, Train Acc: 90.12%, Test Loss: 0.3349, Test Acc: 88.60% Learning Rate: 1e-05Num params: 394538Num operations: 20793856.0\n",
      "TRAINING: Epoch [7/10], Loss: 0.2809, Accuracy: 90.2620, Learning rate: 1e-05\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Epoch 7: Train Loss: 0.2809, Train Acc: 90.26%, Test Loss: 0.3364, Test Acc: 88.76% Learning Rate: 1e-05Num params: 394538Num operations: 20793856.0\n",
      "TRAINING: Epoch [8/10], Loss: 0.2802, Accuracy: 90.1760, Learning rate: 1e-05\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Epoch 8: Train Loss: 0.2802, Train Acc: 90.18%, Test Loss: 0.3368, Test Acc: 88.63% Learning Rate: 1.0000000000000002e-06Num params: 394538Num operations: 20793856.0\n",
      "TRAINING: Epoch [9/10], Loss: 0.2826, Accuracy: 90.2400, Learning rate: 1.0000000000000002e-06\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Epoch 9: Train Loss: 0.2826, Train Acc: 90.24%, Test Loss: 0.3338, Test Acc: 88.74% Learning Rate: 1.0000000000000002e-06Num params: 394538Num operations: 20793856.0\n",
      "TRAINING: Epoch [10/10], Loss: 0.2799, Accuracy: 90.2720, Learning rate: 1.0000000000000002e-06\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Epoch 10: Train Loss: 0.2799, Train Acc: 90.27%, Test Loss: 0.3332, Test Acc: 88.81% Learning Rate: 1.0000000000000002e-06Num params: 394538Num operations: 20793856.0\n",
      "Training complete my boss\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>▁▁▁▁▁▁▁▁▁▁▁███████▂▂▂</td></tr><tr><td>Num operations</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Num params</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Test Accuracy</td><td>▃▆▃▄▃▆▁▁▆▄▄▆▇▇█▇▅▇▅▇█</td></tr><tr><td>Test Loss</td><td>▇▆███▆▇▇▇▆▆▄▂▃▁▄▂▃▃▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▂▁▂▂▁▃▁▁▁▃▂▅▄▅█▅▇▆▇▇</td></tr><tr><td>Train Loss</td><td>█▇▇▇▇▇▆█▇█▇█▅▅▄▁▂▂▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>0.0</td></tr><tr><td>Num operations</td><td>20793856.0</td></tr><tr><td>Num params</td><td>394538</td></tr><tr><td>Test Accuracy</td><td>88.81</td></tr><tr><td>Test Loss</td><td>0.33317</td></tr><tr><td>Train Accuracy</td><td>90.272</td></tr><tr><td>Train Loss</td><td>0.27985</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mini_resnet18_suite</strong> at: <a href='https://wandb.ai/xueyun02-imt-atlantique/deep-learning-lab3/runs/17kebszc' target=\"_blank\">https://wandb.ai/xueyun02-imt-atlantique/deep-learning-lab3/runs/17kebszc</a><br> View project at: <a href='https://wandb.ai/xueyun02-imt-atlantique/deep-learning-lab3' target=\"_blank\">https://wandb.ai/xueyun02-imt-atlantique/deep-learning-lab3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250319_114410-17kebszc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and training details saved to ./mini_resnet18_suite.pth\n"
     ]
    }
   ],
   "source": [
    "#Refaire 10 epochs sur le modèle sauvegardé\n",
    "def train_suite(data_path , batch_size, learning_rate, \n",
    "          weight_decay, epochs, save_path, \n",
    "          load_path, prate):\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Data preprocessing\n",
    "    normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        normalize_scratch,\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize_scratch,\n",
    "    ])\n",
    "\n",
    "    c10train = CIFAR10(data_path,train=True,download=True,transform=transform_train)\n",
    "    c10test = CIFAR10(data_path,train=False,download=True,transform=transform_test)\n",
    "\n",
    "    trainloader = DataLoader(c10train,batch_size=batch_size,shuffle=True)\n",
    "    testloader = DataLoader(c10test,batch_size=batch_size)\n",
    "\n",
    "    # Charger le modèle groupe_8.pth\n",
    "    model = mini_ResNet18(groups=8)\n",
    "    model_checkpoint = torch.load(load_path)\n",
    "    #Enlever keys contenant total dans state_dict\n",
    "    model_checkpoint['model_state_dict'] = {k: v for k, v in model_checkpoint['model_state_dict'].items() if 'total' not in k}\n",
    "    \n",
    "    model.load_state_dict(model_checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "    # Initialize Weights & Biases\n",
    "    dict = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"epochs\": epochs,\n",
    "        \"prate\": prate,\n",
    "    }\n",
    "    wandb.init(project=\"deep-learning-lab3\", config=dict , name=\"mini_resnet18_suite\", job_type=\"training_test\")\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Training accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_train += predicted.eq(labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        print(f\"TRAINING: Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct_test += predicted.eq(labels).sum().item()\n",
    "                total_test += labels.size(0)\n",
    "\n",
    "        test_accuracy = 100 * (correct_test / total_test)\n",
    "        test_loss= test_loss / len(testloader)\n",
    "\n",
    "        scheduler.step(test_loss / len(testloader))\n",
    "\n",
    "        # number of parameters\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        # number operations\n",
    "        input_tensor = torch.randn(1, 3, 32, 32).to(device)\n",
    "        flops, _ = profile(model, inputs=(input_tensor,))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_accuracy:.2f}%, \"\n",
    "              f\"Test Loss: {test_loss:.4f}, \"\n",
    "              f\"Test Acc: {test_accuracy:.2f}% \"\n",
    "              f\"Learning Rate: {optimizer.param_groups[0]['lr']}\"\n",
    "              f\"Num params: {num_params}\"\n",
    "              f\"Num operations: {flops}\"\n",
    "              \n",
    "              )\n",
    "        wandb.log({\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Train Accuracy\": train_accuracy,\n",
    "            \"Test Loss\": test_loss,\n",
    "            \"Test Accuracy\": test_accuracy,\n",
    "            \"Learning Rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"Num params\": num_params,\n",
    "            \"Num operations\": flops\n",
    "        })\n",
    "        \n",
    "        \n",
    "        \n",
    "    print(\"Training complete my boss\")\n",
    "    \n",
    "    # Save model along with training hyperparameters\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epochs\": epochs,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"learning_rate\": learning_rate,\n",
    "    }\n",
    "    wandb.finish()\n",
    "    model.half()\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Model and training details saved to {save_path}\")\n",
    "\n",
    "train_suite('/opt/img/effdl-cifar10/', 64, 0.00001, 0.0005, 10, './mini_resnet18_suite.pth', './mini_resnet18.pth', 0.2)\n",
    "            "
   ]
=======
   "outputs": [],
   "source": []
>>>>>>> 8ad10bb10db37c88a459b6a381184c0ac753cdfa
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
